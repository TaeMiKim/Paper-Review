---
description : Been Kim et al. / Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) / PMLR-2018  
---

# **Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)** 



## **1. Background**  

**Explainable AI (XAI)**

``Interpretation ("Interprerability")of an ML model`` can be seen as function $g:E_{m}→E_{h}$.  
$E_{m}$ : vector space corresponding to data such as input features  
$E_{h}$ : vector space corresponding to a set of human-interpretable concepts

- Class Activation Map (CAM)
- Layer-wise Relevance Propagation (LRP)
- LIME
- Concept Activation Vectors (CAV)


## **2. Problem Definition**  

- Most ML models operate on features such as pixel values, that do not correspond to high-level concepts that humans easily understand.  
- Model’s internal values (e.g., neural activations) can seem incomprehensible.

#### Contribution 
- ``Accessibility``: Requires little to no ML expertise of user.
- ``Customization``: Adapts to any concept (e.g., gender) and is not limited to concepts considered during training.
- ``Plug-in readiness``: Works without any retraining or modification of the ML model.
- ``Global quantification``: Can interpret entire classes or sets of examples with a single quantitative measure, and not just explain individual data inputs.


## **3. Method**  

Please write the methodology author have proposed.  
We recommend you to provide example for understanding it more easily.  

## **4. Experiment**   

### **Experiment setup**  
* Dataset  
* baseline  
* Evaluation Metric  

### **Result**  




## **5. Conclusion**  



---  
## **Author Information**  

* Author name  
    * Affiliation  
    * Research Topic

## **6. Reference & Additional materials**  

Please write the reference. If paper provides the public code or other materials, refer them.  

* Github Implementation  
* Reference  

